Tags: [[__Mathematics]], [[_Probability]]

# Notation and shorthands
This document uses notation and shorthands which are described here - [[Mathematical general notations and shorthands|link]].
# Introduction
Conditional probability distribution function of a random variable ([[Random variable|link]]) (can be a multivariate ([[Multivariate random variable|link]]) and continuous, discrete or mixed ([[Types of random variables|link]])) represents its probability distribution measure ([[Probability distribution|link]]) using another, chosen measure ([[Measure|link]]):
$$
P_{Y \mid X}(A \mid x) = \int_A p_{Y \mid X}(y \mid x) d\mu(y)
$$
where:
- $P_X$ - Probability distribution measure
- $p_X$ - Probability distribution function
- $\mu$ - Our chosen measure
- $\int d\mu$ - Lebesgue integral ([[Lebesgue integral|link]])

Together with that measure, conditional probability distribution function can be used to calculate conditional probabilities of events generated by random variables ([[Event generated by a random variable|link]]).

We usually use the Lebesgue measure ([[Lebesgue measure|link]]) for continuous random variables, the Counting measure ([[Counting measure|link]]) for discrete ones and product of both measures ([[Product measure|link]]) for a mix of continuous and discrete random variables.

Conditional probability distribution function might not exist. It exists only when conditional probability distribution $P_{Y\mid X}$ is absolutely continuous with respect to some measure $\mu$.

Conditional probability distribution function is defined for a fixed value $x$ which represents the condition $X = x$. For different values $x$ we have different conditional probability distribution functions.
# Formal definition
Let's assume that:
- $(\Omega, \mathcal{F}, P)$ - Probability space ([[Probability space (events, probability measure)|link]])
- We have random variables (can be a multivariate and discrete, continuous or mixed):
	- $X: (\Omega, \mathcal{F}) \rightarrow (\mathcal{X}, \mathcal{B}_{\mathcal{X}})$
	- $Y: (\Omega, \mathcal{F}) \rightarrow (\mathcal{Y}, \mathcal{B}_{\mathcal{Y}})$
- $\mu$ is a measure
- Conditional probability distribution $P_{Y \mid X}(\cdot \mid x)$ is absolutely continuous with respect to the measure $\mu$
- Both measures $\mu$ and $P_{Y \mid X}$ are defined on $(\mathcal{Y}, \mathcal{B}_{\mathcal{Y}})$ 

Then, there exists such a probability distribution function function
$$
\large p_{Y \mid X} (\cdot \mid x): \mathcal{Y} \rightarrow [0, \infty)
$$
such that for every measurable set $A \subseteq \mathcal{B}_{\mathcal{Y}}$:
$$
\begin{align}
\text{1) } & p_{Y \mid X}(y \mid x) \ge 0,\ \forall y \\
\text{2) } & P_{Y \mid X}(A \mid x) = \int_A p_{Y \mid X}(y \mid x) d\mu(y) \\
\text{3) } & P_{Y \mid X}(\mathcal{Y} \mid x) = 1
\end{align}
$$

Existence of $p_{Y \mid X}$ is guaranteed by the Radon-Nikodym theorem and it is denoted as:
$$
p_{Y \mid X}(y \mid x) = \frac{dP_{Y \mid X}(\cdot \mid x)}{d\mu}(y)
$$
It is known as Radon-Nikodym derivative of $P_{Y \mid X}$ with respect to $\mu$.

Here $y$ is a function argument and $dP_{Y \mid X}(\cdot \mid x)$ represents a measure for established $x$ which represents the condition $X = x$. 

For different values $x$ we have different conditional probability distribution functions.
# Naming conventions
The conditional probability distribution function $p_{Y \mid X}$ is also called:
- **Conditional probability mass function** if $Y$ is a discrete random variable
- **Conditional probability density function** if $Y$ is a continuous random variable

Additionally, we use phrases:
- '**Joint conditional probability distribution**' if $Y = (Y_1, \ldots, Y_n)$ is a multivariate variable which consists of all the considered random variables
- '**Marginal conditional probability distribution**' if $Y = (Y_1, \ldots, Y_k)$ is only a subset of all the considered random variables ([[Marginal probability distribution|link]])
# Multivariate random variable
To see how the equations and notation from the formal definition look like in case of multivariate random variables, refer to the document here - [[Conditional probability distribution function - Multivariate random variables]].
## Multivariate random variable vs set of random variables
It is worth to highlight, as described here - [[Multivariate random variable vs set or random variables|link]], that considering a single multivariate random variable:
$$
Y = (Y_1, \ldots, Y_n): (\Omega, \mathcal{F}) \rightarrow (\mathcal{Y}_1 \times \ldots \times \mathcal{Y}_n,\ \mathcal{B}_{\mathcal{Y}_1} \otimes \ldots \otimes \mathcal{B}_{\mathcal{Y}_n})
$$
Is equivalent to considering a set of different random variables:
$$
\large
Y_i: (\Omega, \mathcal{F}) \rightarrow (\mathcal{Y}_i, \mathcal{B}_{\mathcal{Y}_i})
$$
Those concepts are equivalent and all the formulas and definitions works the same.

For example, probability distribution function $p_{Y \mid X}$ of a single, multivariate random variable $Y = (Y_1, \ldots, Y_n)$ is equivalent to a joint probability distribution function of its components $\Large p_{Y_{1:n} \mid X_{1:m}}$.
# Different types of random variables
To see how the equations and notation using a conditional probability distribution function looks like in case of different types of random variables, refer to the documents listed below:
- Continuous variables - [[Conditional probability distribution function - Continuous random variables]]
- Discrete variables - [[Conditional probability distribution function - Discrete random variables]]
- Mix of continuous and discrete variables - [[Conditional probability distribution function - Mix of continuous and discrete random variables]]
# Parametric probability distribution function
Sometimes probability distribution function $p_{Y \mid X}$ might be parametric. In that case, we write:
$$
p_{Y \mid X}(y \mid x ; \theta)
$$
where $\theta$ is a set of parameters.
# Alternative notation
Sometimes we might come across alternative annotations:
- For the distribution function $p_{Y \mid X}(y \mid x ; \theta)$:
	- $p_\theta(y \mid x)$ 
	- $p(y \mid x; \theta)$ 
- For the event $P(Y = j \mid X = x ; \theta)$ (where $Y$ is discrete):
	- $p_\theta(y = j \mid x)$ 
	- $p(y = j \mid x; \theta)$ 
	- $P(Y = j \mid x ; \theta)$ 
	- $P(j \mid x ; \theta)$

This notation might be in some cases misleading. For example in the equation:
$$
p(y \mid x) = \sum_{i=1}^n \mathbb{1}\{y = y_i\} p(y = y_i \mid x)
$$
on the left hand side $y$ is a function argument, while on the right hand side $y$ in the $p(y = y_i \mid x)$ is not the same argument, we don't assign to it value of the function's argument $y$.
# Relation to joint probability
Probability and conditional probability distribution functions are related to each other as described here - [[Relation between conditional and joint probability distribution functions]].
# Marginalization
By using marginalization, we can obtain a probability distribution function for a subset of all considered random variables.

For example, if we have $n$ variables $Y_1, \ldots, Y_n$, we can use their probability distribution function $\Large p_{Y_{1:n} \mid X_{1:m}}$ to obtain probability distribution function for $k$ variables out of those: $\Large p_{Y_{1:k} \mid X_{1:m}}$.

More information about it can be found here - [[Marginal conditional probability distribution function]]
# Related topics
1. [[Mathematical general notations and shorthands]]
2. [[Random variable]]
3. [[Probability distribution]]
4. [[Measure]]
5. [[Lebesgue measure]]
6. [[Types of random variables]]
7. [[Counting measure]]
8. [[Probability space (events, probability measure)]]
9. [[Multivariate random variable]]
10. [[Product measure]]
11. [[Cartesian product]]
12. [[Fubini's theorem (multiple integrals)]]

#Mathematics #Probability 