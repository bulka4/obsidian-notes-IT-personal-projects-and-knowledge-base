Tags: [[__Mathematics]], [[_Probability]]

# Introduction
Let $(\Omega, \mathcal{F}, P)$ be a probability space ([[Probability space (events, probability measure)|link]]) and $X: (\Omega, \mathcal{F}) \rightarrow (\mathcal{X}, \mathcal{B}_{\mathcal{X}})$ be a random variable ([[Random variable|link]]).

In probability theory, when we talk about events, we usually mean events generated by random variables. An event generated by a random variable is:
$$
X^{-1}(B) = \{\omega \in \Omega \mid X(\omega) \in B\} \in \mathcal{F},\quad B \in \mathcal{B}_{\mathcal{X}}
$$
So that is a set of samples (experiment outcomes) $\omega \in \Omega$ for which a random variable returns a specific value (which belongs to the set $B$ in this case).

Probability of this event is denoted as:
$$
P_X(B) = P(X \in B) = P(X^{-1}(B)) = P(\{\omega \in \Omega \mid X(\omega) \in B\})
$$
where $P_X$ is a probability distribution ([[Probability distribution|link]]) (a measure that assigns probabilities to events (sets)).

That's why we need the measurability condition from the definition of a random variable:
$$
\forall B \in \mathcal{B}_{\mathcal{X}},\ X^{-1}(B) = \{\omega \in \Omega \mid X(\omega) \in B\} \in \mathcal{F}
$$
It assures that events generated by a random variable are measurable ([[Measurable set|link]]) (they belong to $\mathcal{F}$).
# Probability of an event - notation
In short, we write probabilities of events generated by a random variable in such a way:
$$
\begin{align}
 & P(X \in B) = P(\{\omega: X(\omega) \in B\}) \\
 & P(X = x) = P(X \in \{x\}) = P(\{\omega: X(\omega) = x\})
\end{align}
$$
for $B, \{x\} \in \mathcal{B}_{\mathcal{X}}$.
# Representing events using a random variable
When we have an event:
$$
\{\omega_i\}_{i=1}^n \in \mathcal{F},\quad \omega_i \in \Omega
$$
it can be represented as an outcome of a random variable $X$ for those values $\omega_i$ :
$$
\{\omega_i\}_{i=1}^n = \{\omega \in \Omega \mid X(\omega) \in B\} \in \mathcal{F}
$$
For a some set $B \in \mathcal{B}_{\mathcal{X}}$.

Outcome of a random variable $X(\omega)$ is called an observation \ realization of that random variable for the experiment outcome $\omega$.
# Example
For example, when tossing a coin, the possible outcomes are heads and tails: $\Omega = \{H, T\}$. 

If we define a random variable $X$ such that $X(H) = 1$ and $X(T) = 0$, then: 
$$
P(H) = P(\{\omega : X(\omega) = 1\}) = P(X = 1) = P(X \in \{1\})
$$
# Related topics
1. [[Probability space (events, probability measure)]]
2. [[Random variable]]
3. [[Probability distribution]]
4. [[Measurable set]]

#Mathematics #Probability 