Tags: [[_Mathematical_analysis]]

# Introduction
When using gradient-based optimization algorithms, like gradient descent ([[Gradient Descent - ML|link]]) or Adam ([[Adam optimizer|link]]), for training ML models ([[Training machine learning models - Introduction|link]]), we might encounter an exploding / vanishing gradient problem ([[Vanishing - exploding gradients|link]]).

To help with this, we can use one of the following techniques:
- Gradient clipping - [[Gradient control - Gradient clipping|link]]
- Gradient normalization - [[Gradient control - Gradient normalization|link]]

#MathematicalAnalysis 