Tags: [[_ML_model_inference]]
#MLModelInference 

# Introduction
Latency and throughput are metrics used to measure effectiveness of serving LLMs:
- Latency - Time to the first generated token and time between subsequent generated tokens
- Throughput - Number or inputs processed per second