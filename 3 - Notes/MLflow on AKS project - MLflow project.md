Tags: [[_My_projects]]
#MyProjects 

# Introduction
MLflow project files are saved in the mlflow_project folder in this repo. Here is shortly described how it works:
- Using the 'MLflow project' Helm chart we can deploy a Job which runs this project in a local mode.
- We save project files to run in a File share in Azure Storage Account and mount it to the Job running the project

In the below subsections we can find more details about it:
- **MLflow project Helm chart** - How the Helm chart works
- **MLpoject file** - How the MLproject file looks like, what entrypoints it contains and how they work
- **How to run multiple projects**
- **values.yaml** - How to prepare the values.yaml file for the Helm chart
## MLflow project Helm chart
In the docker > helm_charts > mlflow_project folder we have saved Helm chart used for running the MLflow project. It runs this project as a Job.

How to run a project using this chart:
- **MLflow run command**
    - In the values.yaml file in this chart we specify a command to run the MLflow project.
    - We run the project in a local mode since environment is prepared in the Docker image used for the Job running the project.
    - In this command we specify:
        - Which project to run
        - Which entrypoint to execute and with which parameters
        - Which experiment to assign the run to
- **Deploying a Job**
    - When we deploy this chart, it creates a Job which runs our MLflow project.
- **Mounted project files**
    - File share in Azure Storage Account created by Terraform is mounted to the Job running the project.
    - MLflow project files which we will run should be saved in this File share.
    - The path to the project to run, which we provide in the run command in the values.yaml file, should point to the project saved in the mounted File share.

Other important aspects of how this chart works:
- **Service Account**
    - When deploying a Job running the MLflow project, it uses the Service Account created by the 'MLflow setup' Helm chart.
    - This Service Account is used by the Job to read the Secret which contains value for authentication when pulling image from ACR.
- **Docker image**
    - It pulls a Docker image from ACR prepared when starting the container for interacting with AKS (more info in the 'Build and push to ACR Docker images' section).
    - It contains environment for running the project (Python installed and all the libraries used in the project)
## MLpoject file
In the MLproject file we have two entrypoints defined:
- train:
    - Run the train.py script
    - Train a model on a fake data generated in this script
    - Save the trained model in the artifact store in a specified experiment
- evaluate:
    - Run the evaluate.py script
    - Load the model from the latest run from the experiment specified by the experiment_name parameter
    - Evaluate the model
    - Log metrics in a new experiment (this experiment's name will be the same as the one specified by the experiment_name parameter but with the '_eval' suffix)

So if we want to train a model and then evaluate it, then:
- Run the train entrypoint with specified expieriment name
- Run the evaluate entrypoint with the experiment_name parameter set to the name of the experiment used for training

Instead of evaluating the latest trained model, we could save in a file a run ID of that trained model, read it in the script for evaluation and evaluate a model with this specific run ID.
## How to run multiple projects
When we install the chart for running MLflow project multiple time, we need to make sure that created Jobs will have a unique name.

After installing the chart, it will create a Job called '\<realese-name\>-\<chart-name\>', where:
- \<release-name\> is set up during installing the chart using command `helm install \<release-name\> \<project-path\> -n \<namespace\>`
- \<chart-name\> is set up in the Chart.yaml file

So in order to maintain unique names for created Jobs, we need to use different release name each time we install the chart.
## values.yaml
The values.yaml file is generated by Terraform (more info in the 'Files generated by Terraform (using template files)' section).