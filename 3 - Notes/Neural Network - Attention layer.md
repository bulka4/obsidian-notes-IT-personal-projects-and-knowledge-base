Tags: [[__Machine_Learning]]

# Introduction
Attention layer is a type of a neural network layer ([[Neural Network|link]]) which allows model to focus on the most relevant parts of the input when producing an output. 

It takes as an input a set of $T$ tokens which are 1-D tensors ([[Types of numeric inputs for ML models|link]]):
$$
[x_1, \ldots, x_T], \quad x_i \text{ - 1-D tensor (vector)}
$$
and transforms them into a new set of $T$ 1-D tensors:
$$
[y_1, \ldots, y_T]
$$
Input tokens can be for example words or elements of a feature map generated by a convolution layer ([[Neural Network - Convolution plus Attention layer|link]]).

Token $y_j$ is a mix of information assigned to all the input tokens $x_i$ with different weights assigned, indicating how relevant they are to this token.

So that means, that each input token $x_i$ has an impact on how output tokens $y_j$ for other input tokens $x_j$ will look like. 

Or we can say, that:
- When generating the output token $y_j$ for the input token $x_j$, we take into consideration all the other input tokens $x_i$ (context)
- Input token $x_j$ pays attention to other input tokens $x_i$ (that's why it is called an attention layer)
# How it works - Intuition
Attention layer takes as an input a set of tokens, which are vectors (1-D tensors ([[Types of numeric inputs for ML models|link]])) and changes those tokens into new ones. So the output is a set of transformed input tokens, in the same amount.

All the input tokens has an impact on values of each new token generated by an attention layer.

For example, if the input is a sentence, where each word is represented as a vector (word embedding ([[Word2vec (word embeddings)|link]])):

"rabbit"   -> vector_1
"on"        -> vector_2
"a"          -> vector_3
"field"     -> vector_4
"was"      -> vector_5
"shot"     -> vector_6
"by"        -> vector_7
"a"          -> vector_8
"hunter" -> vector_9

so an input is a set of 9 vectors, then output will be also set of 9 vectors and all the input words will have an impact on how the output vector corresponding to 'rabbit' will look like.

Maybe vector for 'rabbit' will be converted into something more similar to a 'prey', since in this context, the rabbit is a prey of the hunter.

So the entire input sentence might be converted into a new set of tokens with a meaning similar to the sentence 'a prey was hunted at their home by a predator'.

But the output will have the same number of tokens as the input and it will be just a set of vectors, those vectors don't have to be valid words.
# How it works - Calculations
Here we describe mathematically what calculations attention layer performs to generate the output.

We divide this section into two subsections:
- Calculating a single output token - How to calculate a single output token $y_i$ 
- Calculations with a matrix notation - How to calculate all the output tokens $Y = [y_1, \ldots, y_T]$ using matrix variables
## Calculating a single output token
This section describes how to calculate a single output token $y_i$ .

Input for an attention layer is:
$$
\large
X = [x_1, \ldots, x_T] \in \mathbb{R}^{T \times d}, \quad x_i \in \mathbb{R}^d
$$
which is a set of vectors (tokens), that is a 2-D tensor (matrix) of shape $(T, d)$, where:
- $T$ - Input length (sequence length)
- $d$ - Dimension of each token in a sequence

Output is also of shape $(T, d)$:
$$
\large
Y = [y_1, \ldots, y_T] \in \mathbb{R}^{T \times d}, \quad y_i \in \mathbb{R}^d
$$

We use matrices with trainable weights:
$$
\large
W_q, W_k, W_v \in \mathbb{R}^{d \times d}
$$
to calculate three values - $Q, K$ and $V$ (Query, Key and Value) for each input token $x_i$ :
$$
\large
Q_i = x_i W_q, \quad
K_i = x_i W_k, \quad
V_i = x_i W_v, \quad
Q_i, K_i, V_i \in \mathbb{R}^d
$$

Then, attention layer calculates **scores**:
$$
\large
\text{score}_{ij} = Q_i K_j^T
$$
which indicates how related are input tokens $x_i$ and $x_j$ (how much token $x_i$ attends to the token $x_j$).

Then, we scale those scores:
$$
\large 
\text{score}_{ij}^{\text{scaled}} = \frac{Q_i K_j^T}{\sqrt{d}}
$$
and use a softmax function to convert them into **weights** with value in the range $[0, 1]$:
$$
\large
\alpha_{ij} = \frac{\exp( \text{score}_{ij}^{\text{scaled}} )} 
{ \sum_{k=1}^T \exp( \text{score}_{ik}^{\text{scaled}} ) }
\in [0,1]
$$
also softmax makes sure, that $\large \sum_{j} \alpha_{ij} = 1$ .

The output is weighted sum of all the input tokens:
$$
\large
y_i = \sum_{j=1}^T \alpha_{ij} V_j \in \mathbb{R}^{d}
$$
So it is a mix of Values V of all the input tokens, with assigned weights indicating how relevant is the input token $j$ to the input token $i$.
## Calculations with a matrix notation
This section describes how to perform all the calculations using matrix variables.

Let $\large X \in \mathbb{R}^{T \times d}$ be the input.

We use weights, which are trainable parameters (matrices):
$$
\large
W_q, W_k, W_v \in \mathbb{R}^{d \times d}
$$
to calculate values $Q$ (Query), $K$ (Key) and $V$ (Value) per every input token:
$$
\large
Q = X W_q, \quad K = X W_k, \quad V = X W_v, \quad Q, K, V \in \mathbb{R}^{T \times d}
$$
So:
- Each matrix $W_q, W_k, W_v$ is used for every input token
- Each row of $Q, K, V$ corresponds to a specific input token

Then we calculate weights per each pair of input tokens:
$$
\large
A = \text{softmax}\left( \frac{Q K^T} {\sqrt{d}} \right)
\in \mathbb{R}^{T \times T}
$$
so $A_{i,j}$ is a weight for the pair of $i$-th an $j$-th input tokens.

And the final output is:
$$
\large Y = A V = \begin{pmatrix}
y_1 \\
\vdots \\
y_T \\
\end{pmatrix} \in \mathbb{R}^{T \times d}
$$
# Multiple inputs
We can have multiple inputs, up to three $\large X_1, X_2, X_3 \in \mathbb{R}^{T \times d}$. In that case, different inputs can be used to calculate different vectors $Q, K$ and $V$.
# Interpretation
Three values, which we calculate for each input token, can be interpreted as:
- Query - What given token is looking for in other tokens. 
	- It is created in such a way, that the score will be high for other tokens, which are related to the token assigned to this Query.
- Key - What this token offers. This is what other tokens might be looking for.
	- It is created in such a way, that the score will be high for other tokens, which has a Query indicating that they are related to the token with this Key.
- Value - Content, what information given token contains.

So if two tokens are related to each other, we can say that 'they are looking for each other', so they will have such values Q (Query) and K (Key), that they produce a high score:
$$
\large
\text{score}_{ij} = Q_i K_j^T
$$

Each output token is a mix of contents V (Values) of every input token with assigned weights calculated based on the score:
$$
\large
y_i = \sum_{j=1}^T \alpha_{ij} V_j
$$
So if two tokens, $\large x_i, x_j$ are related to each other and they generate a high score, then the content $V_j$ (Value) of one token will have a big impact on how another token is transformed (interpreted).
# Multi-head attention
What we were describing here is a single-head attention but there is also a multi-head version which is described here - [[Neural Network - Multi-Head Attention (MHA) layer]] 
# Masking
Masking in a technique which can be used in an attention layer to cause that some input tokens $x_i$ doesn't impact how output tokens $y_j$ for other input tokens $x_j$ will look like (some input tokens don't pay attention to other input tokens).

It is described here - [[Neural Network Attention layer - Masking]].

#MachineLearning 