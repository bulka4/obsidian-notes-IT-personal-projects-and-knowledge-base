Tags: [[__Mathematics]], [[_Probability]]

# Notation and shorthands
This document uses notation and shorthands which are described here - [[Mathematical general notations and shorthands|link]].
# Introduction
Probability distribution function of a random variable ([[Random variable|link]]) (can be a multivariate ([[Multivariate random variable|link]]) and continuous, discrete or mixed ([[Types of random variables|link]])) represents its probability distribution measure ([[Probability distribution|link]]) using another, chosen measure ([[Measure|link]]):
$$
P_X(A) = P(X \in A) = \int_A p_X(x) d\mu(x)
$$
where:
- $P_X$ - Probability distribution measure
- $p_X$ - Probability distribution function
- $\mu$ - Our chosen measure
- $\int d\mu$ - Lebesgue integral ([[Lebesgue integral|link]])

Together with that measure, probability distribution function can be used to calculate probabilities of events generated by random variables ([[Event generated by a random variable|link]]).

We usually use the Lebesgue measure ([[Lebesgue measure|link]]) for continuous random variables, the Counting measure ([[Counting measure|link]]) for discrete ones and product of both measures ([[Product measure|link]]) for a mix of continuous and discrete random variables.

Probability distribution function might not exist. It exists only when probability distribution $P_X$ is absolutely continuous with respect to some measure $\mu$.
# Formal definition
Let's assume that:
- $(\Omega, \mathcal{F}, P)$ - Probability space ([[Probability space (events, probability measure)|link]])
- $X: (\Omega, \mathcal{F}) \rightarrow (\mathcal{X}, \mathcal{B}_{\mathcal{X}})$ - Random variable (can be a multivariate and discrete, continuous or mixed)
- $\mu$ is a measure
- Probability distribution $P_X$ is absolutely continuous with respect to $\mu$
- Both measures $\mu$ and $P_X$ are defined on $(\mathcal{X}, \mathcal{B}_{\mathcal{X}})$ 

Then, there exists a probability distribution function function 
$$
p_X(\cdot): \mathcal{X} \rightarrow [0, \infty)
$$
such that for every measurable set $A \subseteq \mathcal{B}_{\mathcal{X}}$:
$$
\begin{align}
\text{1) } & p_X(x) \ge 0,\ \forall x \\
\text{2) } & P_X(A) = \int_A p_X(x) d\mu(x) \\
\text{3) } & P_X(\mathcal{X}) = 1
\end{align}
$$

Existence of $p_X$ is guaranteed by the Radon-Nikodym theorem and it is denoted as:
$$
p_X(\cdot) = \frac{dP_X}{d\mu}(\cdot)
$$
It is known as Radon-Nikodym derivative of $P_X$ with respect to $\mu$.
# Naming conventions
The probability distribution function $p_X$ is also called:
- **Probability mass function** if $X$ is a discrete random variable
- **Probability density function** if $X$ is a continuous random variable

Additionally, we use phrases:
- '**Joint probability distribution**' if $X = (X_1, \ldots, X_n)$ is a multivariate variable which consists of all the considered random variables
- '**Marginal probability distribution**' if $X = (X_1, \ldots, X_k)$ is only a subset of all the considered random variables ([[Marginal probability distribution|link]])
# Multivariate random variable
To see how the equations and notation from the formal definition look like in case of multivariate random variables, refer to the document here - [[Probability distribution function - Multivariate random variables]].
## Multivariate random variable vs set of random variables
It is worth to highlight, as described here - [[Multivariate random variable vs set or random variables|link]], that considering a single multivariate random variable:
$$
X = (X_1, \ldots, X_n): (\Omega, \mathcal{F}) \rightarrow (\mathcal{X}_1 \times \ldots \times \mathcal{X}_n,\ \mathcal{B}_{\mathcal{X}_1} \otimes \ldots \otimes \mathcal{B}_{\mathcal{X}_n})
$$
Is equivalent to considering a set of different random variables:
$$
\large
X_i: (\Omega, \mathcal{F}) \rightarrow (\mathcal{X}_i, \mathcal{B}_{\mathcal{X}_i})
$$
Those concepts are equivalent and all the formulas and definitions works the same.

For example, probability distribution function $p_X$ of a single, multivariate random variable $X = (X_1, \ldots, X_n)$ is equivalent to a joint probability distribution function of its components $\Large p_{X_{1:n}}$.
# Different types of random variables
To see how the equations and notations using probability distribution function looks like in case of different types of random variables, refer to the documents listed below:
- Continuous variables - [[Probability distribution function - Continuous random variables]]
- Discrete variables - [[Probability distribution function - Discrete random variables]]
- Mix of continuous and discrete variables - [[Probability distribution function - Mix of continuous and discrete random variables]]
# Parametric probability distribution function
Sometimes probability distribution function $p_X$ might be parametric. In that case, we write:
$$
p_X(x \mid \theta) \text{ or } p_X(x ; \theta)
$$
where $\theta$ is a set of parameters.
# Alternative notation
Sometimes we might come across alternative annotations:
- For the distribution function $p_{Y}(y \mid \theta)$:
	- $p_\theta(y)$ 
	- $p(y \mid \theta)$ 
- For the probability of a specific event $P(Y = j \mid \theta)$ (for a discrete variable):
	- $p_\theta(y = j)$ 
	- $p(y = j \mid \theta)$ 

This notation might be in some cases misleading. For example in the equation:
$$
p(y) = \sum_{i=1}^n \mathbb{1}\{y = y_i\} p(y = y_i)
$$
on the left hand side $y$ is a function argument, while on the right hand side $y$ in the $p(y = y_i)$ is not the same argument, we don't assign to it value of the function's argument $y$.
# Relation to conditional probability
Probability and conditional probability distribution functions are related to each other as described here - [[Relation between conditional and joint probability distribution functions]].
# Marginalization
By using marginalization, we can obtain a probability distribution function for a subset of all considered random variables.

For example, if we have $n$ variables $X_1, \ldots, X_n$, we can use their probability distribution function $\Large p_{X_{1:n}}$ to obtain probability distribution function for $k$ variables out of those: $\Large p_{X_{1:k}}$.

More information about it can be found here - [[Marginal probability distribution function]]
# Related topics
1. [[Mathematical general notations and shorthands]]
2. [[Random variable]]
3. [[Probability distribution]]
4. [[Measure]]
5. [[Lebesgue measure]]
6. [[Types of random variables]]
7. [[Counting measure]]
8. [[Probability space (events, probability measure)]]
9. [[Multivariate random variable]]
10. [[Product measure]]
11. [[Cartesian product]]
12. [[Fubini's theorem (multiple integrals)]]

#Mathematics #Probability 