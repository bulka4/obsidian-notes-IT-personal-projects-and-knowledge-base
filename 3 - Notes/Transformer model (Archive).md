Tags: [[__Machine_Learning]]

# Introduction
Transformer is a model which can be used for both classification and regression tasks.

It is based on Neural Network and Attention.
# Attention
[https://srome.github.io/Understanding-Attention-in-Neural-Networks-Mathematically/](https://srome.github.io/Understanding-Attention-in-Neural-Networks-Mathematically/)

In NLP case this methos catches important fragments of a text and remembers it.
# How Transformer with multi-head attention works in text generation
Here I explain how a transformer model with a multi head attention is used for generating a text. Below we can see a transformer model architecture.
![[2 - Images/Transformer/Screenshot 1.png]]

V, K and Q shown in the slide with an attention are so called values, keys and queries which are matrices. In case of building a chat bot those matrices represents sentences. In attention in encoder V, K and Q are the same. The same is for the first attention in decoder .

The ‘linear’ operation shown in a picture indicates a linear projection. That means that we are multiplying V, K and Q by matrices with weights W_v, W_k and W_q (I am not sure but I think that we are also adding a bias to those values except for multiplying by weights). Those weights are a trainable parameters of a model. V, K and Q multiplied by weights are the input for scaled dot product attention.

We multiply V, K and Q by h different sets of matrices W_v, W_k and W_q and they are a seperate inputs for scaled dot product attention. Then we concatenate all the results from scaled dot product attention and again multiply by a matrix with weights to get a final output.
$$
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
$$
Where
$$
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
$$

## Annotations
- X will be representing a question which we ask to our chat bot
- Y will be representing an answer. 
They are both matrices where each row represents a single word. 
$X_{i, j}$ indicates a value in the i-th row and j-th column. In order to change words into vectors we can use word embeddings models.

## How a transformer generates an answer for a question
- Y at the begining is a matrix with only zeros.
- $X, Y -> transformer(X, Y) = v_1$ -> $v_1$ is a vector representing the first generated word. We assign $v_1$ to $Y[0]$.
- $X, Y -> transformer(X, Y) = v_2$ -> Now $Y$ doesn’t consist only from zeros because $Y[0] = v_1$. Now we assign $v_2$ to $Y[1]$.
- We continue doing this until model generates a special ‘End’ token which indicates that this is the end of a sentence

During a training our $Y$ doesn’t consist only from zeros so before we pass it to a transformer model we need to mask (convert to zero or -inf) all the words from $Y$ which were not generated by a model yet. This masking operation is being done within an attention function.
- X, Y -> mask all rows in Y -> $transformer(X, Y) = v_1$
- X, Y -> mask all rows in Y except for the first row -> $transformer(X, Y) = v_2$
- X, Y -> mask all rows in Y except for the first two rows -> $transformer(X, Y) = v_3$
We continue doing this until we reach the end of Y

Then we compare all generated words $v_1, v_2, ...$ with the real values from $Y$.
## More details about transformer’s operations
Transformer function:
```
x // question sentence
y // answer sentence
def transformer(x, y, mask):
// changing words into a vectors using an embedding model
x = embedding(x) 
x = positional_encoding(x)

// we use an encoder N times
for i in range(N):
x = encoder(x)

y = embedding(y)
y = positional_encoding(y)

// we use a decoder M times
for i in range(M):
y = decoder(y, x, mask)

return y
```

Decoder function:
```
def decoder(y, encoder_output, mask):
query, key, value = y
output = mha(value, key, query) // multi head attention
output = Norm(y + output) // normalization

value, key = encoder_output
query = output
output = mha(value, key, query, mask = mask) // masked attention
output = Norm(y + output)
output = Norm(output + feed_forward(output))

return output
```

In the multi head attention function mha() argument mask indicates which values from variables ‘value’, ‘key’ and ‘query’ will be masked (converted to 0 or -inf). For example if mask = 2 then all rows of ‘value’, ‘key’ and ‘query’ except for the first two rows will be converted to 0 or -inf.

