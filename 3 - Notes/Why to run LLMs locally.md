Tags: [[_ML_model_inference]]
#MLModelInference 

# Introduction
Running LLMs locally instead of using external APIs has the following benefits:
- Regulations and privacy - Our data doesn't leave our infrastructure
- Performance (latency) - We want to have model close to our app so sending data over network doesn't take too much time
- Cost - Running it locally can be cheaper if we use LLM a lot
- Flexibility - We have more control over model's behavior, we can change it however we want